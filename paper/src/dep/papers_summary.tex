\subsection{BayesFlow: Learning Complex Stochastic Models With Invertable Neural Networks \cite{radev2020bayesflowlearningcomplexstochastic}}
\textbf{Problem}: We have the standart Bayesian setup: model with parameters $\theta$ and data $x$. We want to estimate the posterior $p(\theta \mid x)$. From Bayes theorem we have
$$
p(\theta \mid x) \propto p(x \mid \theta) p(\theta)
$$
The problem is that in some cases (namely, likelihood-free cases) right-hand side is intractible because we cannot evaluate the $p(x  \mid  \theta)$, but we can sample from it, i.e.
$$
x_i \sim p(x \mid \theta) \iff x_i = g(\theta, \xi_i),  \xi_i \sim p(\xi)
$$
\textbf{Solution}: Introducing normalizing flow that converts prior into Gaussian 

$$
\theta \sim p(\theta \mid x) \iff \theta = f^{-1}_{\varphi}(z; x), z \sim N(z \mid 0, \mathbb{I})
$$
and considering right loss function we can now learn a summary and inference NN and it will work much more faster for all $\theta$'s and $x$'s
\subsubsection*{Q$\&$A}

\begin{enumerate}
    \item How does the noise $\xi$ selection affects the result? 
        \textbf{Preanswer}: it depends also on simulation we use, and, of course, 
        it does matter what noise we'll choose, because it changes the $p(x)$ and so $p(x \mid \theta)$
    \item Why do we use Gaussian in normalizing flow?
    \item Why don't we minimize the reverse KL divergence? \textbf{Answer}: it is also an option, which is 
    considered in \cite{Murphy2012MachineL}, Chapter 21 and according to \cite{zammitmangion2024neuralmethodsamortizedinference}: \guillemetleft \textit{minimizing the reverse KL divergence leads 
    to approximate distributions that are under-dispersed and that tend to concentrate mass on a single mode 
    of the target distribution, whereas minimizing the forward KL divergence leads to ones that are over-dispersed
    and that cover all modes of the target distribution}\guillemetright. 
    Both approaches are ubiquitious, but forward KL is easier to implement and it is likelihood-free in contrast to reverse KL. 
\end{enumerate}


\subsection{Neural Methods for Amortized Inference \cite{zammitmangion2024neuralmethodsamortizedinference}}
% \subsubsection*{Problem}
% \subsubsection*{Solution}
They introduce Bayes risk as the common case of loss function in \cite{radev2020bayesflowlearningcomplexstochastic}, where it was the KL divergence. 
blah-blah
Minimizing KL divergence vs reverse KL divergence:
\todo[size=\tiny]{different setups and different method} 

Summary networks \todo[size=\tiny]{stopped reading here} 

\subsubsection*{Q$\&$A}

\begin{enumerate}
    \item \textit{Average optimality} and what is it, and why do we use it? 
\end{enumerate}



\subsection{Flow Matching Guide and Code \cite{lipman2024flowmatchingguidecode}}

\subsubsection*{Q$\&$A}

\begin{enumerate}
    \item Prove that conditional optimal transport is really linear, i.e.
    \[
    p_t(x) = \int p_{t \mid 1}(x \mid x_1) q(x_1) dx_1
    \]
    where $p_{t \mid 1}(x \mid x_1) = \mathcal{N}(x \mid t x_1, (1-t)^2 I)$ and 
    \[
    X_t \sim p_t \iff X_t = t X_1 + (1 - t)X_0
    \]
    \item Verify that if $u_t$ generates $p_t$ then they satisfy the Continiuty Equation:
    \[
    \derivative{t} p_t(x) + \Div(p_t u_t) (x) = 0
    \]
    \begin{proof}
        We have $p_t(x) = [\psi_{t\#}p_0](x) = p_0(\psi_{t}^{-1}(x))|J_x(\psi_{t}^{-1})(x)|$. Note that
        \[
        p_{t + s} = [\psi_{s\#}p_t](x)
        \]
        because
        \begin{multline*}
        [\psi_{s\#}p_t](x) = p_t (\psi_s^{-1} (x)) |J_x (\psi_s^{-1}) (x)| = [\psi_{t\#}p_0] (\psi_s^{-1}(x)) |J_x(\psi_s^{-1})(x)| = \\
        = p_0(\psi_{t}^{-1} (\psi_s^{-1} (x))) |J_x(\psi_{t}^{-1})(\psi_s^{-1}(x))| |J_x (\psi_s^{-1}) (x) = \\
        = p_0(\psi_{t+s}^{-1} (x)) |J_x(\psi_{t}^{-1}(\psi_s^{-1}))(x)| = \\
        = p_0(\psi_{t+s}^{-1} (x)) |J_x(\psi_{t+s}^{-1})(x)| = [\psi_{t+s\#}p_0](x)\\
        \end{multline*}
        Then, we can express
        \[
        p_{t+h}(x) - p_t(x) = p_t(\psi_{h}^{-1}(x))|J_x(\psi_{h}^{-1})(x)| - p_t(x)
        \]
        Consider $\psi_{h}(y) = y + \dot{\psi_{0}}(y) h + o(h)$, take $y = \psi_h^{-1}(x) \implies \psi_h^{-1}(x) = x - \dot{\psi_{0}}(\psi_h^{-1}(x)) h + o(h) = x - u_h(x) h + o(h)$. Then 
        \[
        p_t(x) = p_t(x - u_h(x)h + o(h) + u_h(x) h - o(h)) 
        \]
        \[
        p_t(x) = p_t(x - u_h(x)h + o(h)) + \langle \nabla_x p_t, u_h(x) h \rangle + o(h) 
        \]
        \[
        p_t(x - u_h(x)h + o(h)) - p_t(x) = - \langle \nabla_x p_t, u_h(x) h \rangle + o(h) 
        \]
        \[
        p_t(\psi_h^{-1}(x)) - p_t(x) = - \langle \nabla_x p_t, u_h(x) h \rangle + o(h) 
        \]
        As long as $\psi_h(x) \to id$ as $h \to 0$ we have 
        \[
        p_{t+h}(x) - p_t(x) = -\langle \nabla_x p_t, u_h(x) h \rangle + o(h) 
        \]
        
        
    \end{proof}

\end{enumerate}


\subsection{Approximate Bayesian Computation in Population Genetic, \cite{abcinpopulationgenetics}}

\subsubsection*{Q$\&$A}

\begin{enumerate}

    \item Prove the formula for local-linear regression
    \item Provide formulas for multidim local-linear regression + code examples
        \begin{enumerate}
            \item Make class LLRegression with methods
            \item Train and test it in various situations (different $\delta$'s, try other kernels instead of $K_\Delta$)
            \item Compare to analytic solutions and MCMC 
        \end{enumerate}

\end{enumerate}